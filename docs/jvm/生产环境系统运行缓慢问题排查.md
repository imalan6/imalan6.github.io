# 生产环境系统运行缓慢问题排查

## 主要问题

线上环境可能出现系统突然运行缓慢，甚至可能导致线上系统不可用。如果遇到这样的情况，首先需要导出`jstack`日志和内存信息，然后重启系统，保证系统的可用性。这种情况可能的原因有：

- 代码中某个位置读取数据量较大，导致系统内存耗尽，从而导致`Full GC`次数过多，系统缓慢；

- 代码中存在耗时计算，导致 CPU 占用过高，系统运行缓慢；

以上两种是出现频率最高的情况，可能直接导致系统不可用。另外几种情况也可能导致系统运行缓慢：

- 代码某个位置有阻塞性操作，导致该功能调用比较耗时，但这样情况比较随机；

- 某个线程由于某种原因进入`WAITING`状态，此时该功能整体不可用，但无法复现；

- 由于锁使用不当，使得多个线程进入死锁状态，导致系统整体运行比较缓慢。

对于这几种情况，通过查看 CPU 和系统内存是无法检查出具体问题的，因为可能 CPU 和系统内存使用情况都不高，但是系统却很慢。遇到这些情况，只有分析系统日志来排查。

## Full GC次数过多

相对来说，这种情况是最容易出现的，尤其是新功能上线时。对于`Full GC`较多的情况，其主要有如下两个特征：

- 线上多个线程 CPU 占用率都超过了 100%，通过`jstack`命令可以看到这些线程主要是垃圾回收线程；

- 通过`jstat`命令监控`GC`情况，可以看到`Full GC`次数非常多，且次数在不断增加。

首先，我们使用`top`命令查看系统 CPU 的占用情况，如下是系统 CPU 较高的一个情况：

```bash
top - 08:31:10 up 30 min, 0 users, load average: 0.73, 0.58, 0.34
KiB Mem: 2046460 total, 1923864 used, 122596 free, 14388 buffers
KiB Swap: 1048572 total, 0 used, 1048572 free. 1192352 cached Mem

PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND
9 root 20 0 2557160 288976 15812 S 98.0 14.1 0:42.60 java
```

从结果可以看出，有一个进程为 9 的 java应用，它的 CPU 占用率达到了 98.8%，这是很不正常的情况。然后使用如下命令查看下该进程的各个线程运行情况：

```bash
top -Hp 9
```

该进程下的各个线程运行情况如下：

![2.png](https://i.loli.net/2021/03/12/1zUHkiXcTMV6K8Z.png)

可以看到进程为9的 Java 程序中各个线程的 CPU 占用情况，然后通过`jstack`命令查看线程 id 为 10 的线程为什么耗费 CPU 最高。需要注意的是，在`jsatck`命令展示的结果中，线程 id 都转换成了十六进制形式，可以用如下命令转换：

```bash
printf "%x\n" 10

a
```

十六进制形式的线程 id 的为 0xa，然后在`jstack`命令输出结果中查找对应的线程信息：

![3.png](https://i.loli.net/2021/03/12/glKzrQ7cHsqxuUR.png)

找到`nid=0xa`的线程，这里`nid`的意思就是操作系统线程 id 的意思。可以看到线程名称是`VM Thread`，而`VM Thread`指的就是垃圾回收的线程。这里我们基本上可以确定，当前系统缓慢的原因主要是垃圾回收过于频繁，导致`GC`停顿时间较长。通过如下命令可以查看`GC`的情况：

![4.png](https://i.loli.net/2021/03/12/fPH5xDr8ETJSYvG.png)

可以看到，这里`FGC`指的是`Full GC`数量，高达6793，而且还在不断增长。从而进一步证实了是由于内存溢出导致的系统缓慢。那么确认了内存溢出，但是如何查看是哪些对象导致的内存溢出呢，可以`dump`出内存日志，然后通过`eclipse`的`MAT`工具进行查看，如下是其展示的一个对象树结构：

![5.png](https://i.loli.net/2021/03/12/iO3VqcYgIQCsmJn.png)

经过`MAT`工具分析，基本上能确定内存中主要是哪个对象比较消耗内存，然后找到该对象的创建位置，进行处理即可。这里主要是`PrintStream`最多，但是其内存消耗量也只有 12.2%，也就是说，其还不足以导致大量的`Full GC`，此时我们需要考虑另外一种情况，就是代码或者第三方依赖的包中有显示的`System.gc()`调用。这种情况我们查看`dump`内存得到的文件即可判断，因为其会打印`GC`原因：

```bash
[Full GC (System.gc()) [Tenured: 262546K->262546K(349568K), 0.0014879 secs] 262546K->262546K(506816K), [Metaspace: 3109K->3109K(1056768K)], 0.0015151 secs] [Times: user=0.00 sys=0.00, real=0.01 secs]
[GC (Allocation Failure) [DefNew: 2795K->0K(157248K), 0.0001504 secs][Tenured: 262546K->402K(349568K), 0.0012949 secs] 265342K->402K(506816K), [Metaspace: 3109K->3109K(1056768K)], 0.0014699 secs] [Times: user=0.00
```

比如这里第一次`GC`是由于`System.gc()`的显示调用导致的，而第二次`GC`则是`JVM`主动发起的。总结来说，对于`Full GC`次数过多，主要有以下两种原因：

- 代码中一次获取了大量的对象，导致内存溢出，此时可以通过`eclipse`的`MAT`工具查看内存中有哪些对象比较多；

- 内存占用不高，但是`Full GC`次数比较多，此时可能存在显示调用`System.gc()`导致`GC`次数过多，可以通过添加`-XX:+DisableExplicitGC`参数来禁用`JVM`对显示`GC`的响应。

## CPU占用过高

CPU 过高可能是系统频繁地进行`Full GC`，导致系统缓慢。而我们平常也肯能遇到比较耗时的计算，导致 CPU 过高的情况，此时查看方式其实与上面的非常类似。首先我们通过`top`命令查看当前 CPU 消耗过高的进程是哪个，从而得到进程 id；然后通过`top -Hp <pid>`来查看该进程中有哪些线程 CPU 过高，一般超过 80%就是比较高的，80% 左右是合理情况。这样我们就能得到 CPU 消耗比较高的线程 id。接着通过该 线程 id 的十六进制表示在`jstack`日志中查看当前线程具体的堆栈信息。

在这里就可以区分导致 CPU 过高的原因具体是`Full GC`次数过多还是代码中有比较耗时的计算了。如果是`Full GC`次数过多，那么通过`jstack`得到的线程信息会是类似于`VM Thread`之类的线程，而如果是代码中有比较耗时的计算，那么我们得到的就是一个线程的具体堆栈信息。如下是一个代码中有比较耗时的计算，导致 CPU 过高的线程信息：

![0.png](https://i.loli.net/2021/03/12/38Bw4hmgrXUNZEO.png)

这里可以看到，在请求`UserController`的时候，由于该`Controller`进行了一个比较耗时的调用，导致该线程的 CPU 一直处于100%。我们可以根据堆栈信息，直接定位到`UserController`的34行，查看代码中具体是什么原因导致计算量如此之高。

## 接口耗时

对于这种情况，比较典型的例子就是，访问接口返回很慢，而且由于这样的接口耗时是不定时出现的，这就导致了我们在通过`jstack`命令即使得到了线程访问的堆栈信息，我们也没法判断具体哪个线程是正在执行比较耗时操作的线程。

对于不定时出现的接口耗时严重问题，定位思路如下：首先找到该接口，通过压测工具不断加大访问力度，如果说该接口中有某个位置是比较耗时的，由于访问频率非常高，那么大多数线程都将阻塞在这里，这将导致多个线程具有相同的堆栈日志，然后分析日志就可以定位到该接口中比较耗时的代码的位置。如下是一个代码中有比较耗时的阻塞操作通过压测工具得到的线程堆栈日志：

```bash
"http-nio-8080-exec-2" #29 daemon prio=5 os_prio=31 tid=0x00007fd08cb26000 nid=0x9603 waiting on condition [0x00007000031d5000]
 java.lang.Thread.State: TIMED_WAITING (sleeping)
 at java.lang.Thread.sleep(Native Method)
 at java.lang.Thread.sleep(Thread.java:340)
 at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
 at com.aibaobei.user.controller.UserController.detail(UserController.java:18)

"http-nio-8080-exec-3" #30 daemon prio=5 os_prio=31 tid=0x00007fd08cb27000 nid=0x6203 waiting on condition [0x00007000032d8000]
 java.lang.Thread.State: TIMED_WAITING (sleeping)
 at java.lang.Thread.sleep(Native Method)
 at java.lang.Thread.sleep(Thread.java:340)
 at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
 at com.aibaobei.user.controller.UserController.detail(UserController.java:18)

"http-nio-8080-exec-4" #31 daemon prio=5 os_prio=31 tid=0x00007fd08d0fa000 nid=0x6403 waiting on condition [0x00007000033db000]
 java.lang.Thread.State: TIMED_WAITING (sleeping)
 at java.lang.Thread.sleep(Native Method)
 at java.lang.Thread.sleep(Thread.java:340)
 at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
 at com.aibaobei.user.controller.UserController.detail(UserController.java:18)
```

从上面的日志可以看出，有多个线程都阻塞在了`UserController`的第18行代码，说明这是一个阻塞点，也就是导致该接口比较缓慢的原因，然后分析代码即可。

## 线程WAITING状态

线程处于 waiting 状态是比较罕见的一种情况，但是也是有可能出现的，而且由于其具有一定的“不可复现性”，在排查的时候是非常难以发现的。笔者曾经就遇到过类似的这种情况，具体的场景是，在使用`CountDownLatch`时，由于需要每一个并行的任务都执行完成之后才会唤醒主线程往下执行。而当时我们是通过`CountDownLatch`控制多个线程连接并导出用户的`gmail`邮箱数据，这其中有一个线程连接上了用户邮箱，但是连接被服务器挂起了，导致该线程一直在等待服务器的响应。最终导致我们的主线程和其余几个线程都处于 WAITING 状态。

对于这样的问题，查看过`jstack`日志的读者应该都知道，正常情况下，线上大多数线程都是处于`TIMED_WAITING`状态，而我们这里出问题的线程所处的状态与其是一模一样的，这就非常容易混淆我们的判断。解决这个问题的思路主要如下：

- 通过`grep`命令在`jstack`堆栈日志中找出所有的处于`TIMED_WAITING`状态的线程，将其导出到某个文件中，如`a1.log`，如下是一个导出的日志文件示例：

```bash
"Attach Listener" #13 daemon prio=9 os_prio=31 tid=0x00007fe690064000 nid=0xd07 waiting on condition [0x0000000000000000]
"DestroyJavaVM" #12 prio=5 os_prio=31 tid=0x00007fe690066000 nid=0x2603 waiting on condition [0x0000000000000000]
"Thread-0" #11 prio=5 os_prio=31 tid=0x00007fe690065000 nid=0x5a03 waiting on condition [0x0000700003ad4000]
"C1 CompilerThread3" #9 daemon prio=9 os_prio=31 tid=0x00007fe68c00a000 nid=0xa903 waiting on condition [0x0000000000000000]
```

- 等待一段时间之后，比如10s，再次对`jstack`日志进行`grep`，将其导出到另一个文件，如`a2.log`，结果如下所示：

```bash
"DestroyJavaVM" #12 prio=5 os_prio=31 tid=0x00007fe690066000 nid=0x2603 waiting on condition [0x0000000000000000]
"Thread-0" #11 prio=5 os_prio=31 tid=0x00007fe690065000 nid=0x5a03 waiting on condition [0x0000700003ad4000]
"VM Periodic Task Thread" os_prio=31tid=0x00007fe68d114000nid=0xa803waiting on condition
```

- 重复步骤2，待导出 3~4 个文件之后，我们对导出的文件进行对比，找出其中在这几个文件中一直都存在的用户线程，这个线程基本上就可以确认是包含了处于等待状态有问题的线程。因为正常的请求线程是不会在20~30s之后还是处于等待状态的。
- 经过排查得到这些线程之后，我们可以继续对其堆栈信息进行排查，如果该线程本身就应该处于等待状态，比如用户创建的线程池中处于空闲状态的线程，那么这种线程的堆栈信息中是不会包含用户自定义的类的。这些都可以排除掉，而剩下的线程基本上就可以确认是我们要找的有问题的线程。通过其堆栈信息，我们就可以得出具体是在哪个位置的代码导致该线程处于等待状态了。

这里需要说明的是，我们在判断是否为用户线程时，可以通过线程最前面的线程名来判断，因为一般的框架的线程命名都是非常规范的，我们通过线程名就可以直接判断得出该线程是某些框架中的线程，这种线程基本上可以排除掉。而剩余的，比如上面的`Thread-0`，以及我们可以辨别的自定义线程名，这些都是我们需要排查的对象。

经过上面的方式进行排查之后，我们基本上就可以得出这里的`Thread-0`就是我们要找的线程，通过查看其堆栈信息，我们就可以得到具体是在哪个位置导致其处于等待状态了。如下示例中则是在`SyncTask`的第8行导致该线程进入等待了。

```bash
"Thread-0" #11 prio=5 os_prio=31 tid=0x00007f9de08c7000 nid=0x5603 waiting on condition [0x0000700001f89000]
 java.lang.Thread.State: WAITING (parking)
 at sun.misc.Unsafe.park(Native Method)
 at java.util.concurrent.locks.LockSupport.park(LockSupport.java:304)
 at com.aibaobei.chapter2.eg4.SyncTask.lambda$main$0(SyncTask.java:8)
 at com.aibaobei.chapter2.eg4.SyncTask$$Lambda$1/1791741888.run(Unknown Source)
 at java.lang.Thread.run(Thread.java:748)
```

## 线程死锁

对于死锁，这种情况基本上很容易发现，因为`jstack`堆栈信息可以很方便检查出死锁，并且在日志中打印具体的死锁线程信息。如下是一个产生死锁的一个`jstack`日志示例：

![1.png](https://i.loli.net/2021/03/12/d8lem9W6SxvwY72.png)

可以看到，在`jstack`堆栈日志的底部，其直接帮我们分析了日志中存在哪些死锁，以及每个死锁的线程堆栈信息。这里有两个用户线程分别在等待对方释放锁，而被阻塞的位置都是在`ConnectTask`的第5行，此时我们就可以直接定位到该位置，并且进行代码分析，从而找到产生死锁的原因。

## 小结

本文主要讲解了线上可能出现的五种导致系统缓慢的情况，详细分析了每种情况产生时的现象。根据现象可以通过哪些方式定位得到是这种原因导致的系统缓慢。简要的说，可以分为如下步骤：

1）通过`top`命令查看 CPU 占用率高的进程，然后通过`top -Hp <pid>`命令查看当前进程的各个线程运行情况，找出`CPU`过高的线程之后，再将线程 id 转换为十六进制形式，最后在`jstack`日志中查看该线程主要的工作。这里又分为两种情况：

- 如果是用户线程，则通过查看该线程的堆栈信息定位具体是哪处代码运行比较消耗`CPU`；

- 如果该线程是`VM Thread`线程，则通过`jstat -gcutil <pid> <period> <times>`命令监控当前系统的`GC`状况，然后通过`jmapdump:format=b,file=<filepath> <pid>`导出系统当前的内存数据。并通过`eclipse`的`MAT`内存分析工具查看具体是什么对象比较消耗内存，最后再修改相关代码；详见 [生产环境CPU 100%解决思路](jvm/生产环境CPU100解决思路.md)。

2）如果通过`top`命令看到`CPU`并不高，并且系统内存占用率也比较低。此时就可以考虑是否是由于另外三种情况导致的问题。具体的可以根据具体情况分析：

3）如果是接口调用比较耗时，并且是不定时出现，则可以通过压测的方式加大阻塞点出现的频率，并通过`jstack`查看堆栈信息，找到系统阻塞点；

4）如果是某个功能突然出现停滞状况，这种情况也无法复现，此时可以通过多次导出`jstack`日志的方式对比哪些用户线程是一直都处于等待状态，这些线程就是可能存在问题的线程；

5）如果通过`jstack`查看到死锁线程，则可以检查导致线程死锁的具体资源并处理相应的问题。